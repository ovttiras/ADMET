{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18466480-482f-4086-98f5-c7ead0a545ed",
   "metadata": {},
   "source": [
    "# TDC ADMET, Caco-2_Wang Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c86952a-47f8-4e3d-a22e-36ecfe207217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cheminformatics\n",
    "import rdkit.Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# logging\n",
    "import tqdm\n",
    "\n",
    "# data preprocessing\n",
    "import sklearn.impute\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# modeling\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# metrics\n",
    "import sklearn.metrics\n",
    "\n",
    "from tdc.single_pred import ADME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee27599-741d-4e64-a737-e71ec9094266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = ADME(name = 'Caco2_Wang')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ea5838-436a-44e0-add0-549ff410ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_descriptor_columns(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use rdkit to get descriptors of each drug in the `data` df.\n",
    "    Return a Pandas DataFrame with the descriptors as columns in the df and .\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the Drug column\n",
    "    assert 'Drug' in data.columns, \"'Drug' must be a column in the input DataFrame.\"\n",
    "    drugs = data['Drug']\n",
    "    y = data['Y']\n",
    "    \n",
    "    # Get the descriptors for each drug\n",
    "    print(\"Calculating descriptors...\")\n",
    "    descriptors = []\n",
    "    for drug, target in tqdm.tqdm(zip(drugs, y)):\n",
    "        descriptor = Descriptors.CalcMolDescriptors(\n",
    "            rdkit.Chem.MolFromSmiles(drug)\n",
    "        )\n",
    "        descriptor['Drug'] = drug\n",
    "        descriptor['Y'] = target\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    # Make a dataframe for the descriptors\n",
    "    df = pd.DataFrame(descriptors)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_data(\n",
    "    data: pd.DataFrame, \n",
    "    imputer=sklearn.impute.SimpleImputer(missing_values=np.nan, strategy='mean'),\n",
    "    fit_imputer=True,\n",
    "    scaler_X=sklearn.preprocessing.RobustScaler(),\n",
    "    scaler_y=sklearn.preprocessing.RobustScaler(),\n",
    "    fit_scaler=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Imputes missing values.\n",
    "    Scales feature data.\n",
    "\n",
    "    Returns a tuple X, y of scaled feature data and target data.\n",
    "    \"\"\"\n",
    "\n",
    "    col_array = np.array(data.columns)\n",
    "\n",
    "    # extract just the feature data\n",
    "    X = data[col_array[~np.isin(col_array, ['Drug_ID', 'Drug', 'Y'])]].to_numpy()\n",
    "    \n",
    "    # extract the target data\n",
    "    y = np.array(data['Y']).reshape(-1,1)\n",
    "    \n",
    "    # impute missing data\n",
    "    if imputer is not None:\n",
    "        if fit_imputer:\n",
    "            X = imputer.fit_transform(X)\n",
    "        else:\n",
    "            X = imputer.transform(X)\n",
    "\n",
    "    # scale the feature data\n",
    "    if scaler_X is not None:\n",
    "        if fit_scaler:\n",
    "            X = scaler_X.fit_transform(X)\n",
    "            y = scaler_y.fit_transform(y)\n",
    "        else:\n",
    "            X = scaler_X.transform(X)\n",
    "            y = scaler_y.transform(y)\n",
    "\n",
    "\n",
    "\n",
    "    return X, y, imputer, scaler_X, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6010f6-e6e6-4c7d-ae76-f608faef7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "637it [00:07, 83.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 94.33it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, imputer, scaler_X, scaler_y = preprocess_data(\n",
    "    add_descriptor_columns(split['train'])\n",
    ")\n",
    "X_val, y_val, _, _, _ = preprocess_data(\n",
    "    add_descriptor_columns(split['valid']),\n",
    "    imputer=imputer, fit_imputer=False,\n",
    "    scaler_X=scaler_X, scaler_y=scaler_y,\n",
    "    fit_scaler=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa9b3c7-0671-4583-b443-efd93e7fbab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Stage 1: 100%|██████████| 336/336 [00:41<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры (Stage 1): {'C': 1000, 'epsilon': 0.1, 'gamma': 0.0001}\n",
      "MAE (Stage 1): 0.2899285437538299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Stage 2: 100%|██████████| 125/125 [00:14<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры (Stage 2): {'C': 800.0, 'epsilon': 0.08000000000000002, 'gamma': 0.0001}\n",
      "MAE (Stage 2): 0.286875443153019\n",
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n",
      "Лучшие параметры (Stage 3 RandomizedSearchCV): {'gamma': 0.00019721892245187355, 'epsilon': 0.09575757575757578, 'C': 1512.8446244837814}\n",
      "MAE (Stage 3): 0.14170463745811263\n",
      "✅ Лучшая модель сохранена в: model/Cacao2\\best_model_svm.pkl\n",
      "✅ Параметры сохранены в: model/Cacao2\\best_params.json\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid, RandomizedSearchCV\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Папка для сохранения модели и параметров\n",
    "model_dir = \"model/Cacao2\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ------------------ Stage 1: грубый поиск ------------------ #\n",
    "params_grid_stage1 = params_grid_stage1 = {\n",
    "    \"C\": [10 ** i for i in range(-1, 6)],         # [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    \"gamma\": [10 ** i for i in range(-7, 1)],     # [1e-7, 1e-6, ..., 1e0]\n",
    "    \"epsilon\": [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]  # более широкий диапазон\n",
    "}\n",
    "\n",
    "\n",
    "best_score = float('inf')\n",
    "best_set = {}\n",
    "best_model = None\n",
    "\n",
    "for param_set in tqdm(ParameterGrid(params_grid_stage1), desc=\"Grid Search Stage 1\"):\n",
    "    model = SVR(kernel=\"rbf\", **param_set)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    y_val_pred_tmp = model.predict(X_val)\n",
    "    score_MAE = mean_absolute_error(y_val, y_val_pred_tmp)\n",
    "\n",
    "    if score_MAE < best_score:\n",
    "        best_score = score_MAE\n",
    "        best_set = param_set\n",
    "        best_model = model\n",
    "\n",
    "print(\"Лучшие параметры (Stage 1):\", best_set)\n",
    "print(\"MAE (Stage 1):\", best_score)\n",
    "\n",
    "# ------------------ Stage 2: уточнение ------------------ #\n",
    "C_best = best_set[\"C\"]\n",
    "gamma_best = best_set[\"gamma\"]\n",
    "eps_best = best_set[\"epsilon\"]\n",
    "\n",
    "params_grid_stage2 = {\n",
    "    \"C\": [C_best * f for f in [0.5, 0.8, 1.0, 1.2, 2.0]],\n",
    "    \"gamma\": [gamma_best * f for f in [0.5, 0.8, 1.0, 1.2, 2.0]],\n",
    "    \"epsilon\": [max(1e-4, eps_best * f) for f in [0.5, 0.8, 1.0, 1.2, 2.0]]\n",
    "}\n",
    "\n",
    "for param_set in tqdm(ParameterGrid(params_grid_stage2), desc=\"Grid Search Stage 2\"):\n",
    "    model = SVR(kernel=\"rbf\", **param_set)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    y_val_pred_tmp = model.predict(X_val)\n",
    "    score_MAE = mean_absolute_error(y_val, y_val_pred_tmp)\n",
    "\n",
    "    if score_MAE < best_score:\n",
    "        best_score = score_MAE\n",
    "        best_set = param_set\n",
    "        best_model = model\n",
    "\n",
    "print(\"Лучшие параметры (Stage 2):\", best_set)\n",
    "print(\"MAE (Stage 2):\", best_score)\n",
    "\n",
    "# ------------------ Stage 3: RandomizedSearchCV ------------------ #\n",
    "param_distributions = {\n",
    "    \"C\": np.logspace(np.log10(best_set[\"C\"] * 0.5), np.log10(best_set[\"C\"] * 2), 100),\n",
    "    \"gamma\": np.logspace(np.log10(best_set[\"gamma\"] * 0.5), np.log10(best_set[\"gamma\"] * 2), 100),\n",
    "    \"epsilon\": np.linspace(max(1e-4, best_set[\"epsilon\"] * 0.5), best_set[\"epsilon\"] * 2, 100)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVR(kernel=\"rbf\"),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,              # 20 случайных комбинаций\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=[(np.arange(len(X_train)), np.arange(len(X_val)))],  # имитация train/val split\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(\n",
    "    np.vstack((X_train, X_val)), \n",
    "    np.hstack((y_train.ravel(), y_val.ravel())))\n",
    "\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_set = random_search.best_params_\n",
    "best_score = -random_search.best_score_\n",
    "\n",
    "print(\"Лучшие параметры (Stage 3 RandomizedSearchCV):\", best_set)\n",
    "print(\"MAE (Stage 3):\", best_score)\n",
    "\n",
    "# ------------------ Сохраняем модель и параметры ------------------ #\n",
    "model_path = os.path.join(model_dir, \"best_model_svm.pkl\")\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "params_path = os.path.join(model_dir, \"best_params.json\")\n",
    "with open(params_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_score\": best_score,\n",
    "        \"best_params\": best_set\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(\"✅ Лучшая модель сохранена в:\", model_path)\n",
    "print(\"✅ Параметры сохранены в:\", params_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f450840-30c0-43ac-bb3a-d62050c35f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728it [00:08, 90.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [00:02, 89.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728it [00:07, 91.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [00:02, 87.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728it [00:08, 89.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [00:02, 86.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728it [00:08, 88.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [00:02, 87.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 5:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728it [00:08, 90.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [00:02, 82.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caco2_wang': [0.374, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "\n",
    "group = admet_group(path='data/')\n",
    "predictions_list = []\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    benchmark = group.get('Caco2_Wang') \n",
    "    predictions = {}\n",
    "    name = benchmark['name']\n",
    "\n",
    "    # используем весь train_val для обучения\n",
    "    train_val, test = benchmark['train_val'], benchmark['test']\n",
    "\n",
    "    print(f\"Seed {seed}:\")\n",
    "\n",
    "    # ---------------- Предобработка ---------------- #\n",
    "    X_train, y_train, imputer, scaler_X, scaler_y = preprocess_data(\n",
    "        add_descriptor_columns(train_val)\n",
    "    )\n",
    "    X_test, y_test, _, _, _ = preprocess_data(\n",
    "        add_descriptor_columns(test),\n",
    "        imputer=imputer, fit_imputer=False,\n",
    "        scaler_X=scaler_X, fit_scaler=False,\n",
    "        scaler_y=scaler_y\n",
    "    )\n",
    "\n",
    "    # ---------------- SVM (SVR) ---------------- #\n",
    "    # Пример параметров — при необходимости замените или подберите через CV\n",
    "    svr_model = SVR(**best_set)\n",
    "    # Входные y в preprocess_data, как в вашем XGBoost-окружении, вероятно, уже масштабированы\n",
    "    svr_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # ---------------- Предсказания ---------------- #\n",
    "    y_pred_test_scaled = svr_model.predict(X_test)  # 1D array\n",
    "    # inverse_transform ожидает 2D, поэтому reshape\n",
    "    y_pred_test = scaler_y.inverse_transform(\n",
    "        y_pred_test_scaled.reshape(-1, 1)\n",
    "    ).reshape(-1)\n",
    "\n",
    "    # сохраняем по TDC-шаблону\n",
    "    predictions[name] = y_pred_test\n",
    "    predictions_list.append(predictions)\n",
    "\n",
    "# ---------------- Оценка ---------------- #\n",
    "results = group.evaluate_many(predictions_list)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0cb39-08f4-449f-8aac-ce93bee5e434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
