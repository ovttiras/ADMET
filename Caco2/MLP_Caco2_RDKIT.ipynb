{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18466480-482f-4086-98f5-c7ead0a545ed",
   "metadata": {},
   "source": [
    "# TDC ADMET, Caco-2_Wang Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c86952a-47f8-4e3d-a22e-36ecfe207217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cheminformatics\n",
    "import rdkit.Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# logging\n",
    "import tqdm\n",
    "\n",
    "# data preprocessing\n",
    "import sklearn.impute\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# modeling\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# metrics\n",
    "import sklearn.metrics\n",
    "\n",
    "from tdc.single_pred import ADME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee27599-741d-4e64-a737-e71ec9094266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = ADME(name = 'Caco2_Wang')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ea5838-436a-44e0-add0-549ff410ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys, Descriptors\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def add_descriptor_columns(\n",
    "    data: pd.DataFrame,\n",
    "    morgan_radius=2,\n",
    "    morgan_nbits=1024,\n",
    "    use_maccs=False,\n",
    "    use_morgan=False,\n",
    "    use_rdkit=True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate descriptors (MACCS, Morgan, RDKit) for each molecule in the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Must contain 'Drug' (SMILES) and 'Y' (target) columns.\n",
    "    morgan_radius : int\n",
    "        Radius for Morgan fingerprints.\n",
    "    morgan_nbits : int\n",
    "        Number of bits for Morgan fingerprints.\n",
    "    use_maccs, use_morgan, use_rdkit : bool\n",
    "        Whether to include MACCS, Morgan, or RDKit descriptors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with selected descriptors + Y + Drug.\n",
    "    \"\"\"\n",
    "\n",
    "    assert 'Drug' in data.columns, \"'Drug' must be a column in the input DataFrame.\"\n",
    "    assert 'Y' in data.columns, \"'Y' must be a column in the input DataFrame.\"\n",
    "\n",
    "    drugs = data['Drug']\n",
    "    y = data['Y']\n",
    "\n",
    "    fps = []\n",
    "    print(\"Calculating descriptors...\")\n",
    "\n",
    "    rdkit_desc = [d for d in Descriptors._descList] if use_rdkit else []\n",
    "\n",
    "    for smi, target in tqdm.tqdm(zip(drugs, y), total=len(drugs)):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        row_parts = []\n",
    "\n",
    "        if mol is None:\n",
    "            if use_maccs:\n",
    "                row_parts.append(np.zeros(167, dtype=int))\n",
    "            if use_morgan:\n",
    "                row_parts.append(np.zeros(morgan_nbits, dtype=int))\n",
    "            if use_rdkit:\n",
    "                row_parts.append(np.zeros(len(rdkit_desc), dtype=float))\n",
    "        else:\n",
    "            if use_maccs:\n",
    "                maccs = np.array(MACCSkeys.GenMACCSKeys(mol))\n",
    "                row_parts.append(maccs)\n",
    "\n",
    "            if use_morgan:\n",
    "                morgan = np.array(GetMorganFingerprintAsBitVect(mol, morgan_radius, nBits=morgan_nbits))\n",
    "                row_parts.append(morgan)\n",
    "\n",
    "            if use_rdkit:\n",
    "                rdkit_vals = []\n",
    "                for name, func in rdkit_desc:\n",
    "                    try:\n",
    "                        rdkit_vals.append(func(mol))\n",
    "                    except Exception:\n",
    "                        rdkit_vals.append(np.nan)\n",
    "                rdkit_vals = np.array(rdkit_vals, dtype=float)\n",
    "                row_parts.append(rdkit_vals)\n",
    "\n",
    "        row = np.concatenate(row_parts + [[target]])\n",
    "        fps.append(row)\n",
    "\n",
    "    # Формируем имена колонок\n",
    "    fp_columns = []\n",
    "    if use_maccs:\n",
    "        fp_columns += [f'maccs_{i}' for i in range(167)]\n",
    "    if use_morgan:\n",
    "        fp_columns += [f'morgan_{i}' for i in range(morgan_nbits)]\n",
    "    if use_rdkit:\n",
    "        fp_columns += [f'rdkit_{name}' for name, _ in rdkit_desc]\n",
    "\n",
    "    fp_columns += ['Y']\n",
    "\n",
    "    df = pd.DataFrame(fps, columns=fp_columns)\n",
    "    df['Drug'] = drugs.values\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "    data: pd.DataFrame, \n",
    "    imputer=sklearn.impute.SimpleImputer(missing_values=np.nan, strategy='mean'),\n",
    "    fit_imputer=True,\n",
    "    scaler_X=sklearn.preprocessing.RobustScaler(),\n",
    "    scaler_y=sklearn.preprocessing.RobustScaler(),\n",
    "    fit_scaler=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Imputes missing values.\n",
    "    Scales feature data.\n",
    "\n",
    "    Returns a tuple X, y of scaled feature data and target data.\n",
    "    \"\"\"\n",
    "\n",
    "    col_array = np.array(data.columns)\n",
    "\n",
    "    # extract just the feature data\n",
    "    X = data[col_array[~np.isin(col_array, ['Drug_ID', 'Drug', 'Y'])]].to_numpy()\n",
    "    \n",
    "    # extract the target data\n",
    "    y = np.array(data['Y']).reshape(-1,1)\n",
    "    \n",
    "    # impute missing data\n",
    "    if imputer is not None:\n",
    "        if fit_imputer:\n",
    "            X = imputer.fit_transform(X)\n",
    "        else:\n",
    "            X = imputer.transform(X)\n",
    "\n",
    "    # scale the feature data\n",
    "    if scaler_X is not None:\n",
    "        if fit_scaler:\n",
    "            X = scaler_X.fit_transform(X)\n",
    "            y = scaler_y.fit_transform(y)\n",
    "        else:\n",
    "            X = scaler_X.transform(X)\n",
    "            y = scaler_y.transform(y)\n",
    "\n",
    "\n",
    "\n",
    "    return X, y, imputer, scaler_X, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6010f6-e6e6-4c7d-ae76-f608faef7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [00:08<00:00, 78.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:01<00:00, 84.79it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, imputer, scaler_X, scaler_y = preprocess_data(\n",
    "    add_descriptor_columns(split['train'])\n",
    ")\n",
    "X_val, y_val, _, _, _ = preprocess_data(\n",
    "    add_descriptor_columns(split['valid']),\n",
    "    imputer=imputer, fit_imputer=False,\n",
    "    scaler_X=scaler_X, scaler_y=scaler_y,\n",
    "    fit_scaler=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa9b3c7-0671-4583-b443-efd93e7fbab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Stage 1: 100%|██████████| 96/96 [00:26<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры (Stage 1): {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate_init': 0.001}\n",
      "MAE (Stage 1): 0.3109600658787877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Stage 2: 100%|██████████| 25/25 [00:05<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры (Stage 2): {'activation': 'tanh', 'alpha': 0.0002, 'hidden_layer_sizes': (50,), 'learning_rate_init': 0.0005}\n",
      "MAE (Stage 2): 0.30556448362100064\n",
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n",
      "Лучшие параметры (Stage 3 RandomizedSearchCV): {'learning_rate_init': 0.0008930016176563258, 'hidden_layer_sizes': (100, 50), 'alpha': 0.0002208179027347626, 'activation': 'tanh'}\n",
      "MAE (Stage 3): 0.09870252538220474\n",
      "✅ Лучшая модель сохранена в: model/Cacao2\\best_model_mlp.pkl\n",
      "✅ Параметры сохранены в: model/Cacao2\\best_params.json\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid, RandomizedSearchCV\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Папка для сохранения модели и параметров\n",
    "model_dir = \"model/Cacao2\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ------------------ Stage 1: грубый поиск ------------------ #\n",
    "params_grid_stage1 = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50,50), (100,50)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"alpha\": [10 ** i for i in range(-5, -1)],  # L2 penalty\n",
    "    \"learning_rate_init\": [10 ** i for i in range(-4, -1)]\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_set = {}\n",
    "best_model = None\n",
    "\n",
    "for param_set in tqdm(ParameterGrid(params_grid_stage1), desc=\"Grid Search Stage 1\"):\n",
    "    model = MLPRegressor(max_iter=1000, random_state=42, **param_set)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    y_val_pred_tmp = model.predict(X_val)\n",
    "    score_MAE = mean_absolute_error(y_val, y_val_pred_tmp)\n",
    "\n",
    "    if score_MAE < best_score:\n",
    "        best_score = score_MAE\n",
    "        best_set = param_set\n",
    "        best_model = model\n",
    "\n",
    "print(\"Лучшие параметры (Stage 1):\", best_set)\n",
    "print(\"MAE (Stage 1):\", best_score)\n",
    "\n",
    "# ------------------ Stage 2: уточнение ------------------ #\n",
    "alpha_best = best_set[\"alpha\"]\n",
    "lr_best = best_set[\"learning_rate_init\"]\n",
    "\n",
    "params_grid_stage2 = {\n",
    "    \"hidden_layer_sizes\": [best_set[\"hidden_layer_sizes\"]],\n",
    "    \"activation\": [best_set[\"activation\"]],\n",
    "    \"alpha\": [alpha_best * f for f in [0.5, 0.8, 1.0, 1.2, 2.0]],\n",
    "    \"learning_rate_init\": [lr_best * f for f in [0.5, 0.8, 1.0, 1.2, 2.0]]\n",
    "}\n",
    "\n",
    "for param_set in tqdm(ParameterGrid(params_grid_stage2), desc=\"Grid Search Stage 2\"):\n",
    "    model = MLPRegressor(max_iter=1000, random_state=42, **param_set)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    y_val_pred_tmp = model.predict(X_val)\n",
    "    score_MAE = mean_absolute_error(y_val, y_val_pred_tmp)\n",
    "\n",
    "    if score_MAE < best_score:\n",
    "        best_score = score_MAE\n",
    "        best_set = param_set\n",
    "        best_model = model\n",
    "\n",
    "print(\"Лучшие параметры (Stage 2):\", best_set)\n",
    "print(\"MAE (Stage 2):\", best_score)\n",
    "\n",
    "# ------------------ Stage 3: RandomizedSearchCV ------------------ #\n",
    "param_distributions = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50,50), (100,50)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"alpha\": np.logspace(np.log10(best_set[\"alpha\"] * 0.5), np.log10(best_set[\"alpha\"] * 2), 50),\n",
    "    \"learning_rate_init\": np.logspace(np.log10(best_set[\"learning_rate_init\"] * 0.5),\n",
    "                                     np.log10(best_set[\"learning_rate_init\"] * 2), 50)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    MLPRegressor(max_iter=1000, random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # 20 случайных комбинаций\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=[(np.arange(len(X_train)), np.arange(len(X_val)))],  # имитация train/val split\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(\n",
    "    np.vstack((X_train, X_val)), \n",
    "    np.hstack((y_train.ravel(), y_val.ravel()))\n",
    ")\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_set = random_search.best_params_\n",
    "best_score = -random_search.best_score_\n",
    "\n",
    "print(\"Лучшие параметры (Stage 3 RandomizedSearchCV):\", best_set)\n",
    "print(\"MAE (Stage 3):\", best_score)\n",
    "\n",
    "# ------------------ Сохраняем модель и параметры ------------------ #\n",
    "model_path = os.path.join(model_dir, \"best_model_mlp.pkl\")\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "params_path = os.path.join(model_dir, \"best_params.json\")\n",
    "with open(params_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_score\": best_score,\n",
    "        \"best_params\": best_set\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(\"✅ Лучшая модель сохранена в:\", model_path)\n",
    "print(\"✅ Параметры сохранены в:\", params_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f450840-30c0-43ac-bb3a-d62050c35f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e0cb39-08f4-449f-8aac-ce93bee5e434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [00:08<00:00, 83.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:02<00:00, 77.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [00:08<00:00, 83.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:02<00:00, 83.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [00:09<00:00, 78.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:02<00:00, 83.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [00:09<00:00, 78.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:02<00:00, 78.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 5:\n",
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [00:08<00:00, 82.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:02<00:00, 77.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caco2_wang': [0.41, 0.036]}\n"
     ]
    }
   ],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "group = admet_group(path='data/')\n",
    "predictions_list = []\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    benchmark = group.get('Caco2_Wang') \n",
    "    predictions = {}\n",
    "    name = benchmark['name']\n",
    "\n",
    "    train_val, test = benchmark['train_val'], benchmark['test']\n",
    "\n",
    "    print(f\"Seed {seed}:\")\n",
    "\n",
    "    # ---------------- Предобработка ---------------- #\n",
    "    X_train, y_train, imputer, scaler_X, scaler_y = preprocess_data(\n",
    "        add_descriptor_columns(train_val)\n",
    "    )\n",
    "    X_test, y_test, _, _, _ = preprocess_data(\n",
    "        add_descriptor_columns(test),\n",
    "        imputer=imputer, fit_imputer=False,\n",
    "        scaler_X=scaler_X,\n",
    "        scaler_y=scaler_y,\n",
    "        fit_scaler=False\n",
    "    )\n",
    "\n",
    "    # ---------------- MLPRegressor ---------------- #\n",
    "    mlp_model = MLPRegressor(max_iter=1000, random_state=seed, **best_set)\n",
    "    mlp_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # ---------------- Предсказания ---------------- #\n",
    "    y_pred_test_scaled = mlp_model.predict(X_test)\n",
    "    y_pred_test = scaler_y.inverse_transform(\n",
    "        y_pred_test_scaled.reshape(-1, 1)\n",
    "    ).reshape(-1)\n",
    "\n",
    "    predictions[name] = y_pred_test\n",
    "    predictions_list.append(predictions)\n",
    "\n",
    "# ---------------- Оценка ---------------- #\n",
    "results = group.evaluate_many(predictions_list)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcaed6d-f86d-408d-a839-14796c5cef64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
